# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2bxgRC2XP14yTHoiBLzwJ9BzR1HV55W
"""

import os
import aisuite as ai
import gradio as gr

# 1. API å®¢æˆ¶ç«¯è¨­å®š (æ”¹ç”¨ç’°å¢ƒè®Šæ•¸è®€å–)
# åœ¨ Hugging Face éƒ¨ç½²æ™‚ï¼Œæˆ‘å€‘æœƒåœ¨ Settings è¨­å®š GROQ_API_KEY
api_key = os.getenv('GROQ_API_KEY')

# åˆå§‹åŒ–æ¨¡å‹åƒæ•¸
PROVIDER = "groq"
IMAGE_MODEL = "llama-3.1-8b-instant"
TEXT_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"

# åˆå§‹åŒ– aisuite å®¢æˆ¶ç«¯
try:
    client = ai.Client()
    print("âœ… aisuite å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    client = None
    print(f"âŒ éŒ¯èª¤ï¼šaisuite åˆå§‹åŒ–å¤±æ•—ï¼š{e}")

# 2. Prompt å„ªåŒ–å‡½æ•¸ (æ²¿ç”¨ä½ çš„é‚è¼¯ï¼Œä½†ç§»é™¤ print è¨ºæ–·ä»¥ä¿æŒä»‹é¢ä¹¾æ·¨)
def optimize_prompt(high_level_concept, ai_type, style, tone):
    if not high_level_concept or high_level_concept.strip() == "":
        return ("### âš ï¸ éŒ¯èª¤ï¼šè«‹åœ¨ã€Œæ¦‚å¿µè¼¸å…¥ã€æ¬„ä½ä¸­è¼¸å…¥å…§å®¹ï¼", "", "")

    if not client or not os.getenv('GROQ_API_KEY'):
        return ("### âŒ åš´é‡éŒ¯èª¤ï¼šAPI Key æœªè¨­å®šï¼Œè«‹åœ¨ Space Settings ä¸­é…ç½® GROQ_API_KEYã€‚", "", "")

    if ai_type == 'åœ–åƒ (Midjourney/Stable Diffusion)':
        target_model = IMAGE_MODEL
        system_prompt = f"""
        **ä½ æ˜¯ä¸€å€‹ä¸–ç•Œç´šçš„åœ–åƒæç¤ºè©å·¥ç¨‹å¸« (Image Prompt Engineer)ã€‚**
        ä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…çš„é«˜å±¤æ¬¡æ¦‚å¿µï¼Œè½‰åŒ–ç‚ºä¸€å€‹**çµæ§‹åŒ–ã€æ¥µåº¦è©³ç›¡**ï¼Œä¸”åŒ…å«å°ˆæ¥­æ”å½±/è—è¡“è¡“èªçš„å–®è¡Œç²¾æº– Promptã€‚
        **è«‹åš´æ ¼éµå¾ªä»¥ä¸‹è¼¸å‡ºçµæ§‹ï¼š**
        [ä¸»é«”æè¿°, å‹•ä½œ/è¡¨æƒ…], [ç’°å¢ƒ/èƒŒæ™¯, å…‰ç·š/æ™¯æ·±], [è—è¡“é¢¨æ ¼: {style}, è—è¡“å®¶], [æŠ€è¡“åƒæ•¸, å“è³ªä¿®é£¾è©]ã€‚
        ä½¿ç”¨è€…æ¦‚å¿µ: {high_level_concept}
        é¢¨æ ¼è¦æ±‚: {style}
        """
    elif ai_type == 'æ–‡å­— (GPT-4/Gemini)':
        target_model = TEXT_MODEL
        system_prompt = f"""
        **ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹ç­–åŠƒå¸«å’ŒæŒ‡ä»¤å„ªåŒ–å°ˆå®¶ã€‚**
        ä½ çš„ä»»å‹™æ˜¯å°‡ä½¿ç”¨è€…çš„é«˜å±¤æ¬¡æ¦‚å¿µï¼Œå„ªåŒ–æˆä¸€å€‹**çµæ§‹æ¸…æ™°ã€ç›®æ¨™æ˜ç¢º**ï¼Œèƒ½è®“ç›®æ¨™ LLM ç”Ÿæˆä¸€ç¯‡é«˜å“è³ªå…§å®¹çš„æŒ‡ä»¤ Promptã€‚
        **è¼¸å‡ºè¦æ±‚ï¼š** ç¢ºä¿ Prompt æŒ‡ä»¤åŒ…å«ã€Œè§’è‰²ã€ã€ã€Œä»»å‹™ã€ã€ã€Œæ ¼å¼ã€å’Œã€Œç´„æŸã€ã€‚
        **é¢¨æ ¼è¦æ±‚ï¼š** {style} é¢¨æ ¼ã€‚
        **èªæ°£è¦æ±‚ï¼š** {tone} èªæ°£ã€‚
        """
    else:
        return ("### âŒ éŒ¯èª¤ï¼šè«‹é¸æ“‡æœ‰æ•ˆçš„ AI è¼¸å‡ºé¡å‹ã€‚", "", "")

    user_concept = f"ä½¿ç”¨è€…çš„é«˜å±¤æ¬¡æ¦‚å¿µæ˜¯ï¼š'{high_level_concept}'ã€‚è«‹åƒ…è¼¸å‡ºæœ€çµ‚å„ªåŒ–å¾Œçš„ Prompt æ–‡æœ¬ï¼Œä¸éœ€è¦ä»»ä½•å¤šé¤˜çš„è§£é‡‹æˆ–å¼•è¨€ã€‚"
    full_model_name = f"{PROVIDER}:{target_model}"

    try:
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_concept}]
        response = client.chat.completions.create(
            model=full_model_name,
            messages=messages,
            temperature=0.6
        )
        final_prompt_text = response.choices[0].message.content
        output_html = f"### âœ… ç”Ÿæˆå®Œæˆï¼ğŸ¯ ç›®æ¨™æ¨¡å‹: **{target_model}**"
        return (output_html, final_prompt_text.strip(), system_prompt.strip())
    except Exception as e:
        return (f"### âŒ API å‘¼å«å¤±æ•—ï¼\néŒ¯èª¤è¨Šæ¯ï¼š{str(e)[:100]}", "", "")

# 3. Gradio UI ä»‹é¢è¨­è¨ˆ
ai_type_options = ['åœ–åƒ (Midjourney/Stable Diffusion)', 'æ–‡å­— (GPT-4/Gemini)']

with gr.Blocks(title="æ™ºæ…§å‹æç¤ºè©å·¥ç¨‹å¸«", theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  æ™ºæ…§å‹æç¤ºè©å·¥ç¨‹å¸« (Intelligent Prompt Engineer)")
    gr.Markdown("æ­¤å·¥å…·åˆ©ç”¨ **Groq LLM** çš„æ¨ç†èƒ½åŠ›ï¼Œå°‡æ‚¨çš„ã€Œé«˜å±¤æ¬¡æ¦‚å¿µã€å„ªåŒ–æˆç²¾æº– Promptã€‚")

    with gr.Tab("ğŸš€ Prompt ç”Ÿæˆå™¨"):
        with gr.Row():
            with gr.Column(scale=4):
                concept_input = gr.Textbox(label="ğŸ“ æ¦‚å¿µè¼¸å…¥", placeholder="ä¾‹å¦‚: ä¸€éš»åœ¨é™é æ˜Ÿç³»æ—…è¡Œçš„ç©¿è‘—å®‡èˆªæœçš„è²“å’ª", lines=4)
                ai_type_selector = gr.Dropdown(choices=ai_type_options, value=ai_type_options[0], label="ğŸ¯ ç›®æ¨™è¼¸å‡º AI é¡å‹")
                style_selector = gr.Textbox(label="ğŸ¨ æœŸæœ›çš„è¼¸å‡ºé¢¨æ ¼", value="å¯«å¯¦é¢¨æ ¼ (Photorealistic)")
                tone_selector = gr.Textbox(label="ğŸ—£ï¸ æœŸæœ›çš„èªæ°£/æƒ…ç·’", value="æ­£å¼/å­¸è¡“ (Formal/Academic)")
                submit_btn = gr.Button("ğŸš€ ç”Ÿæˆç²¾æº– Prompt", variant="primary")
            with gr.Column(scale=6):
                output_header = gr.HTML(value="### ğŸ¯ å„ªåŒ–å¾Œçš„ç²¾æº– Prompt å°‡åœ¨æ­¤è™•é¡¯ç¤º")
                final_prompt_code = gr.Textbox(label="âœ¨ æœ€çµ‚ç²¾æº– Prompt", lines=8, interactive=False)
                meta_prompt_detail = gr.Code(label="âš™ï¸ ä½¿ç”¨çš„ Meta-Prompt", language="markdown", lines=8, visible=False)
                toggle_meta_btn = gr.Button("é¡¯ç¤º/éš±è— Meta-Prompt å°ˆæ¥­ç´°ç¯€", size="sm")

    submit_btn.click(fn=optimize_prompt, inputs=[concept_input, ai_type_selector, style_selector, tone_selector], outputs=[output_header, final_prompt_code, meta_prompt_detail])

    def toggle_visibility(visible):
        return gr.update(visible=not visible)

    toggle_meta_btn.click(fn=toggle_visibility, inputs=[meta_prompt_detail], outputs=[meta_prompt_detail])

# 4. å•Ÿå‹•æœå‹™ (åœ¨é›²ç«¯é‹è¡Œä¸éœ€ share=True)
if __name__ == "__main__":
    demo.launch()